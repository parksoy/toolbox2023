{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xs58EaTvVsZk"
      },
      "source": [
        "### Q1 Data Cleaning: Write a function to handle missing values in a dataset. How would you decide which strategy to use for each column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUuqrkwhVtB7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df is your DataFrame and it contains a column \"column_name\"\n",
        "df['column_name'].replace(np.nan, df['column_name'].mean(), inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCgVnKMSV0cU"
      },
      "source": [
        "### Q2 Data Manipulation: Given a dataset with a date column, write code to extract features such as month, year, day of the week, and whether the date is a holiday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au3wC7bRV3CL"
      },
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'], df['month'], df['day_of_week'] = df['date'].dt.year, df['date'].dt.month, df['date'].dt.dayofweek\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKAo8O4CV3sx"
      },
      "source": [
        "### Q3 Machine Learning: Given a dataset, write code to train a binary classification model. Include preprocessing steps, model training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIQApPyrV6Vj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Ecr3LuWKFm"
      },
      "source": [
        "### Q4 Deep Learning: Explain how you would implement a convolutional neural network for image classification. Write code to define the architecture of the network using a library such as TensorFlow or PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXnyxOdFWMjB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6oDC8adWNMK"
      },
      "source": [
        "### Q5 Natural Language Processing: Write a function that takes a sentence as input and returns a list of all named entities (people, places, organizations) in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PutOnu5rWQCf"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def extract_named_ents(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(extract_named_ents(\"Apple is looking at buying U.K. startup for $1 billion\"))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YL7OVkMoWQhL"
      },
      "source": [
        "### Q6 Time Series Analysis: Given a time series dataset, write code to decompose the series into trend, seasonal, and residual components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je6MgoJtWSaD"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "series = pd.Series([i+np.random.randint(10) for i in range(1,100)])\n",
        "result = seasonal_decompose(series, model='additive', period=1)\n",
        "\n",
        "result.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxnSMxXWSzP"
      },
      "source": [
        "### Q7 SQL: Write a SQL query to find the top 5 customers with the highest total purchase amount in each month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB95ZYoQWUjq"
      },
      "outputs": [],
      "source": [
        "SELECT customer_id, sum(purchase_amount) as total_purchase, MONTH(purchase_date) as month\n",
        "GROUP BY month, customer_id\n",
        "ORDER BY total_purchase DESC\n",
        "LIMIT 5 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4a-RV6CGWVOA"
      },
      "source": [
        "### Q8 Optimization: Implement a gradient descent algorithm from scratch to find the minimum of a simple function (e.g., a quadratic function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABMwzggaWXQ6"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x_start, learning_rate, num_iterations):\n",
        "    x = x_start\n",
        "    for i in range(num_iterations):\n",
        "        gradient = 2*x  # derivative of x^2\n",
        "        x = x - learning_rate * gradient\n",
        "    return x\n",
        "\n",
        "print(gradient_descent(10, 0.1, 100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NOdzBwLXWXsL"
      },
      "source": [
        "### Q9 Statistics: Write a function to perform a hypothesis test comparing the means of two groups. The function should take as input two lists of numbers, and output the p-value of the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNsNjBNfWaQC"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "def t_test(group1, group2):\n",
        "    t_stat, p_val = stats.ttest_ind(group1, group2)\n",
        "    return p_val\n",
        "\n",
        "print(t_test([1, 2, 3, 4, 5], [1, 2, 3, 4, 5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAw9jh5qWarh"
      },
      "source": [
        "### Q10 Data Structure and Algorithms: Implement a data structure to store and retrieve customer data, with efficient methods for the following operations: insert a new customer, delete a customer, look up a customer by ID, and list all customers in order of their total purchase amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU7lJC6uWe2O"
      },
      "outputs": [],
      "source": [
        "class CustomerDatabase:\n",
        "    def __init__(self):\n",
        "        self.customers = dict()\n",
        "    \n",
        "    def insert(self, id, name, purchase_amount):\n",
        "        self.customers[id] = (name, purchase_amount)\n",
        "    \n",
        "    def delete(self, id):\n",
        "        del self.customers[id]\n",
        "    \n",
        "    def lookup(self, id):\n",
        "        return self.customers[id]\n",
        "    \n",
        "    def list_customers(self):\n",
        "        return sorted(self.customers.items(), key=lambda x: x[1][1], reverse=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QekrGAwi9ik9"
      },
      "source": [
        "### Q11: can you get me 10 different deep learning project examples and its codes in Python tensorflow/keras?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zZhHvIzX5qq2"
      },
      "source": [
        "#### Example 1. Image Classification: Use a Convolutional Neural Network (CNN) to classify images in the CIFAR-10 dataset.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAAeRB8L9tnf"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ShqqbYL59uxz"
      },
      "source": [
        "#### Example 2.  Text Classification: Use a Recurrent Neural Network (RNN) or Transformer to classify movie reviews as positive or negative (sentiment analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0IPYVEE-vRl"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([ tf.keras.layers.Embedding(vocab_size, 32), \n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \n",
        "                             tf.keras.layers.Dense(32, activation='relu'), \n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid') ])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0922yw9253"
      },
      "source": [
        "#### Example 3. Object Detection: Implement a model like YOLO (You Only Look Once) to detect and classify objects within an image.\n",
        "\n",
        "Implementing a complex model like YOLO from scratch requires significant effort and expertise in computer vision and deep learning. However, I can provide you with an example code that uses the pre-trained YOLO model from the popular library darknet to perform object detection in images. Here's an example of how you can use the darknet library in Python:\n",
        "\n",
        "First, you need to install the darknet library and download the pre-trained weights for the YOLO model. Instructions for installation can be found in the darknet repository: https://github.com/AlexeyAB/darknet.\n",
        "\n",
        "Once you have installed darknet, you can use the following Python code to perform object detection using YOLO:\n",
        "\n",
        "In this example, make sure to replace \"yolo.cfg\", \"yolo.weights\", and \"coco.data\" with the appropriate paths to the YOLO model configuration file, pre-trained weights file, and the dataset configuration file, respectively. Additionally, replace \"image.jpg\" with the path to the image you want to perform object detection on.\n",
        "\n",
        "The code loads the pre-trained YOLO model using darknet.load_net and darknet.load_meta. It then loads and resizes the input image. After converting the image to the required format, it performs object detection using darknet.detect_image. The resulting detections are processed, and bounding boxes with class labels are drawn on the image. Finally, the image with the detected objects is displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QIZahX9cWgi"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import darknet\n",
        "\n",
        "# Load the pre-trained YOLO model\n",
        "net = darknet.load_net(b\"yolo.cfg\", b\"yolo.weights\", 0)\n",
        "meta = darknet.load_meta(b\"coco.data\")\n",
        "\n",
        "# Load the image\n",
        "image_path = \"image.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Resize the image to the required input size of YOLO\n",
        "resized_image = cv2.resize(image, (darknet.network_width(net), darknet.network_height(net)), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Convert the image to the required YOLO format (RGB, float32)\n",
        "darknet_image = darknet.make_image(darknet.network_width(net), darknet.network_height(net), 3)\n",
        "darknet.copy_image_from_bytes(darknet_image, resized_image.tobytes())\n",
        "\n",
        "# Perform object detection\n",
        "detections = darknet.detect_image(net, meta, darknet_image)\n",
        "\n",
        "# Process the detections\n",
        "for detection in detections:\n",
        "    class_name = detection[0].decode()\n",
        "    confidence = detection[1]\n",
        "    x, y, w, h = detection[2]\n",
        "    x1 = int(x - w / 2)\n",
        "    y1 = int(y - h / 2)\n",
        "    x2 = int(x + w / 2)\n",
        "    y2 = int(y + h / 2)\n",
        "    \n",
        "    # Draw bounding box and label on the image\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(image, f\"{class_name} ({confidence:.2f})\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "# Display the image with detections\n",
        "cv2.imshow(\"Object Detection\", image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "keNN0r4D98s3"
      },
      "source": [
        "#### Example 4. Neural Machine Translation: Build a Sequence-to-Sequence (Seq2Seq) model to translate text from one language to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uP0iOU_cYT3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data.metrics import bleu_score\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Define the fields for source and target languages\n",
        "SRC = Field(tokenize=\"spacy\", tokenizer_language=\"en\", lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "TRG = Field(tokenize=\"spacy\", tokenizer_language=\"de\", lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "# Load and split the Multi30k dataset\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=(\".en\", \".de\"), fields=(SRC, TRG))\n",
        "\n",
        "# Build the vocabulary\n",
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, dropout):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.encoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        enc_output, (enc_hidden, enc_cell) = self.encoder(embedded)\n",
        "        dec_output, _ = self.decoder(enc_output, (enc_hidden, enc_cell))\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n",
        "\n",
        "# Set device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model hyperparameters\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Initialize the model\n",
        "model = Seq2Seq(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Create data iterators\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in iterator:\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZcO6ana-QML"
      },
      "source": [
        "#### Example 5. Generative Adversarial Network (GAN): Train a GAN to generate new images, like creating new artworks or synthesizing human faces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEdFnL-EcaGp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(7 * 7 * 256, input_dim=latent_dim))\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Define the loss functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Define the discriminator loss function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "# Define the generator loss function\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Define the optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Define the training loop\n",
        "def train_gan(generator, discriminator, gan, dataset, epochs, latent_dim, num_examples=16):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in dataset:\n",
        "            # Generate random noise as input to the generator\n",
        "            noise = tf.random.normal([image_batch.shape[0], latent_dim])\n",
        "\n",
        "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "                # Generate images using the generator\n",
        "                generated_images = generator(noise, training=True)\n",
        "\n",
        "                # Compute the discriminator outputs\n",
        "                real_output = discriminator(image_batch, training=True\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LTi4oNrp-SMf"
      },
      "source": [
        "#### Example 6.  Autoencoders: Use autoencoders for anomaly detection in credit card transactions or for reducing the dimensionality of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05wPMmr4ca1F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the credit card transactions dataset\n",
        "data = pd.read_csv('credit_card_transactions.csv')\n",
        "\n",
        "# Separate the features and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 32\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(encoding_dim, activation='relu', input_shape=(input_dim,)))\n",
        "model.add(layers.Dense(input_dim, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the autoencoder\n",
        "model.fit(X_train, X_train, epochs=10, batch_size=128, validation_data=(X_test, X_test))\n",
        "\n",
        "# Use the trained autoencoder for anomaly detection\n",
        "train_predictions = model.predict(X_train)\n",
        "train_mse = np.mean(np.power(X_train - train_predictions, 2), axis=1)\n",
        "train_threshold = np.percentile(train_mse, 95)  # Set a threshold to classify anomalies\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "test_mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
        "\n",
        "# Classify anomalies based on the threshold\n",
        "y_pred = np.where(test_mse > train_threshold, 1, 0)\n",
        "\n",
        "# Evaluate the performance\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EHIIp0pf-VLx"
      },
      "source": [
        "#### Example 7. Reinforcement Learning: Implement a Deep Q-Network (DQN) to teach an agent how to play a video game.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RebWq22jcbsx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Tic-Tac-Toe environment\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3))\n",
        "        self.current_player = 1  # 1: Agent, -1: Opponent\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3))\n",
        "        self.current_player = 1\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        return np.argwhere(self.board == 0)\n",
        "\n",
        "    def make_move(self, row, col):\n",
        "        if self.board[row, col] != 0 or self.game_over:\n",
        "            return False\n",
        "\n",
        "        self.board[row, col] = self.current_player\n",
        "\n",
        "        # Check for a winning move\n",
        "        if self.check_winner():\n",
        "            self.winner = self.current_player\n",
        "            self.game_over = True\n",
        "        # Check for a draw\n",
        "        elif len(self.get_valid_moves()) == 0:\n",
        "            self.winner = 0\n",
        "            self.game_over = True\n",
        "        else:\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_winner(self):\n",
        "        for player in [1, -1]:\n",
        "            if np.any(np.all(self.board == player, axis=0)) or np.any(np.all(self.board == player, axis=1)) or \\\n",
        "                    np.all(np.diagonal(self.board) == player) or np.all(np.diagonal(np.fliplr(self.board)) == player):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def print_board(self):\n",
        "        for row in self.board:\n",
        "            row_str = ['X' if val == 1 else 'O' if val == -1 else ' ' for val in row]\n",
        "            print('|'.join(row_str))\n",
        "            print('-----')\n",
        "        print()\n",
        "\n",
        "# Deep Q-Network (DQN) model\n",
        "class DQN:\n",
        "    def __init__(self):\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.epsilon_min = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.replay_buffer = []\n",
        "\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(64, input_shape=(9,), activation='relu'))\n",
        "        model.add(layers.Dense(64, activation='relu'))\n",
        "        model.add(layers.Dense(9, activation='linear'))\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(np.argwhere(state == 0).flatten())\n",
        "        else:\n",
        "            q_values = self.model.predict(state[np.newaxis, :])[0]\n",
        "            valid_moves = np.argwhere(state == 0).flatten()\n",
        "            valid_q_values = [q_values[i] for i in valid_moves]\n",
        "            return valid_moves[np.argmax(valid_q_values)]\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay_buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions =\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0oMcrZRj-d1u"
      },
      "source": [
        "#### Example 8. Text Generation: Train a model to generate text, such as completing a sentence or writing poetry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3qFSuNWcdqU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"\"\"\n",
        "This is an example text used for training a text generation model. It can be any text that you want to use as input.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the text and create input-output pairs\n",
        "text = input_text.lower()\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "input_sequences = []\n",
        "output_chars = []\n",
        "sequence_length = 100\n",
        "\n",
        "for i in range(len(text) - sequence_length):\n",
        "    input_seq = text[i:i + sequence_length]\n",
        "    output_char = text[i + sequence_length]\n",
        "    input_sequences.append([char_to_index[char] for char in input_seq])\n",
        "    output_chars.append(char_to_index[output_char])\n",
        "\n",
        "# Convert input sequences and output characters to numpy arrays\n",
        "X = np.array(input_sequences)\n",
        "y = np.array(output_chars)\n",
        "\n",
        "# Define the text generation model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(len(chars), 64, input_length=sequence_length),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, batch_size=128, epochs=50)\n",
        "\n",
        "# Generate text using the trained model\n",
        "start_text = \"This is\"\n",
        "num_chars = 200\n",
        "\n",
        "generated_text = start_text.lower()\n",
        "\n",
        "for _ in range(num_chars):\n",
        "    input_seq = [char_to_index[char] for char in generated_text[-sequence_length:]]\n",
        "    input_seq = np.array(input_seq)[np.newaxis, :]\n",
        "    predicted = model.predict(input_seq)\n",
        "    next_index = np.argmax(predicted)\n",
        "    next_char = index_to_char[next_index]\n",
        "    generated_text += next_char\n",
        "\n",
        "print(generated_text)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LgMwCwdQjopk"
      },
      "source": [
        "### Q12: What does data science field deal with? like profit optimization?\n",
        "\n",
        "The field of data science deals with extracting insights and knowledge from large and complex datasets to drive informed decision-making. It encompasses various disciplines such as statistics, mathematics, computer science, and domain expertise to analyze and interpret data.\n",
        "\n",
        "Profit optimization is indeed one of the areas where data science can be applied. In this context, data scientists may employ statistical and machine learning techniques to analyze historical sales data, market trends, customer behavior, and other relevant factors. By understanding the relationships between different variables and identifying patterns, data scientists can develop models and algorithms to optimize business strategies and maximize profits.\n",
        "\n",
        "Specifically, in profit optimization, data science can help with:\n",
        "\n",
        "**[1] Demand forecasting**: By analyzing historical sales data and external factors (e.g., weather, economic indicators), data scientists can build models to forecast future demand. This information can guide decision-making related to production, inventory management, and pricing.\n",
        "\n",
        "**[2] Pricing optimization**: Data scientists can leverage pricing data, customer preferences, competitor information, and market dynamics to develop pricing models that maximize profitability. They may use techniques like price elasticity analysis and A/B testing to determine optimal price points.\n",
        "\n",
        "**[3] Customer segmentation and targeting**: Data science enables businesses to identify different customer segments based on various attributes such as demographics, purchasing behavior, and preferences. This segmentation helps in targeted marketing campaigns and personalized recommendations, thereby increasing the probability of generating higher profits.\n",
        "\n",
        "**[4] Churn prediction and customer retention**: By analyzing customer data and usage patterns, data scientists can build models to predict customer churn or attrition. This information allows businesses to take proactive measures to retain customers by offering personalized incentives, targeted marketing, or improving customer experience.\n",
        "\n",
        "**[5] Optimization of marketing campaigns**: Data science techniques, such as predictive analytics and machine learning algorithms, can be used to optimize marketing campaigns. By analyzing data on customer responses, conversion rates, and marketing channels, data scientists can identify the most effective strategies and allocate marketing budgets more efficiently.\n",
        "\n",
        "These are just a few examples of how data science can contribute to profit optimization. The field is vast, and its applications span across industries, including finance, healthcare, retail, manufacturing, and many others."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dGADb6_EjaZl"
      },
      "source": [
        "##### 12-[1] Demand forecasting: \n",
        "By analyzing historical sales data and external factors (e.g., weather, economic indicators), data scientists can build models to forecast future demand. This information can guide decision-making related to production, inventory management, and pricing.\n",
        "\n",
        "In this example, you would need to have three CSV files: one containing historical sales data (sales_data.csv), one with weather data (weather_data.csv), and another with economic indicators data (economic_data.csv). The sales data should include a date column and a sales column.\n",
        "\n",
        "The code loads the data, preprocesses it, merges the exogenous variables (weather and economic indicators) with the sales data, splits the data into training and testing sets, builds a SARIMAX forecasting model, and generates future demand forecasts. Finally, it evaluates the accuracy of the forecast using the mean absolute percentage error (MAPE).\n",
        "\n",
        "Keep in mind that this is just a basic example, and in practice, you would need to explore more sophisticated models, handle missing data, handle seasonality and trends, perform feature engineering, and conduct thorough model evaluation and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn0WNvF2jiy0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the historical sales data\n",
        "sales_data = pd.read_csv('sales_data.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "sales_data['Date'] = pd.to_datetime(sales_data['Date'])\n",
        "sales_data = sales_data.set_index('Date')\n",
        "\n",
        "# Resample the data to aggregate by a specific time period (e.g., monthly)\n",
        "monthly_sales = sales_data['Sales'].resample('M').sum()\n",
        "\n",
        "# Prepare the exogenous variables (e.g., weather, economic indicators)\n",
        "weather_data = pd.read_csv('weather_data.csv')\n",
        "economic_data = pd.read_csv('economic_data.csv')\n",
        "\n",
        "# Merge exogenous variables with the sales data\n",
        "merged_data = monthly_sales.to_frame().join(weather_data.set_index('Date')).join(economic_data.set_index('Date'))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = merged_data.loc[:'2022-12-31']\n",
        "test_data = merged_data.loc['2023-01-01':]\n",
        "\n",
        "# Build and train the forecasting model (e.g., SARIMAX)\n",
        "model = sm.tsa.SARIMAX(train_data['Sales'], exog=train_data[['Weather', 'EconomicIndicators']], order=(1, 0, 0))\n",
        "results = model.fit()\n",
        "\n",
        "# Forecast future demand\n",
        "forecast = results.get_forecast(steps=len(test_data), exog=test_data[['Weather', 'EconomicIndicators']])\n",
        "forecasted_values = forecast.predicted_mean\n",
        "\n",
        "# Evaluate the accuracy of the forecast (e.g., using mean absolute percentage error)\n",
        "mape = (abs(forecasted_values - test_data['Sales']) / test_data['Sales']).mean() * 100\n",
        "print('Mean Absolute Percentage Error (MAPE): {:.2f}%'.format(mape))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B5eKTzBV4fr8"
      },
      "source": [
        "##### 12-[3] **Pricing optimization** : Data scientists can leverage pricing data, customer preferences, competitor information, and market dynamics to develop pricing models that maximize profitability. They may use techniques like price elasticity analysis and A/B testing to determine optimal price points.\"\n",
        " \n",
        "demonstrates how you can use linear regression to perform price elasticity analysis and optimize pricing based on customer preferences and market dynamics. Please note that this is a basic example, and in practice, you may need to consider additional factors and use more sophisticated models.\n",
        "\n",
        "In this example, you would need two CSV files: one containing the pricing data (pricing_data.csv) and another with the corresponding demand data (demand_data.csv). The pricing data should include a column for the product price, customer preferences, competitor prices, and other relevant features. The demand data should have the corresponding demand or sales figures for each product.\n",
        "\n",
        "The code loads the pricing and demand data, merges them based on the product, and prepares the input features (X) and target variable (y) for the linear regression model. It fits a linear regression model and obtains the coefficients (which represent the impact of each feature on demand). Then, it calculates the price elasticity by dividing the negative coefficient of price (coefficients[0]) by the demand. Finally, it identifies the price with the maximum price elasticity, which represents the optimal price point for maximizing profitability.\n",
        "\n",
        "Keep in mind that this is a basic example, and in practice, you may need to handle additional features, perform feature engineering, handle multicollinearity, evaluate model performance, and consider other factors such as market segmentation, seasonality, and competitor strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFUu_3V95A0G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the pricing and demand data\n",
        "pricing_data = pd.read_csv('pricing_data.csv')\n",
        "demand_data = pd.read_csv('demand_data.csv')\n",
        "\n",
        "# Merge the pricing and demand data\n",
        "merged_data = pricing_data.merge(demand_data, on='Product')\n",
        "\n",
        "# Prepare the input features (price, customer preferences, competitor information, etc.)\n",
        "X = merged_data[['Price', 'CustomerPreference', 'CompetitorPrice']]\n",
        "y = merged_data['Demand']\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Obtain the coefficients of the model\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Calculate price elasticity\n",
        "price_elasticity = -coefficients[0] * merged_data['Price'] / merged_data['Demand']\n",
        "\n",
        "# Optimize pricing by finding the maximum price elasticity\n",
        "optimal_price = merged_data['Price'].loc[price_elasticity.idxmax()]\n",
        "\n",
        "print('Optimal Price: {:.2f}'.format(optimal_price))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
